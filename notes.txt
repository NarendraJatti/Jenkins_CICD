Jenkins is both a Continuous Integration (CI) and Continuous Delivery/Deployment (CD) tool, depending on how you configure and use it



https://www.youtube.com/watch?v=To-KzPB_EnE&t=863s>>good
https://www.youtube.com/watch?v=g1raKOTTGb0

Conitinous Integration(CI)>>packaging the codebuild
C delivery(CD)>>mnanual intervention for deployment(for ex,codebuild manual button)> actual deployment is manual.
C deployment(CD)>>full autiomated deplyment

In Jenkins, checkout refers to the process of retrieving (or checking out) the source code from a version control system (such as Git, SVN, etc.) into the Jenkins workspace, where the pipeline or job is executed.
Jenkins uses the checkout step to pull the code so it can perform the build, test, and deployment tasks specified in the pipeline.
If the Jenkins file is in the source code repository, why is checkout not required?
When you create a Jenkins pipeline using a Jenkinsfile stored in your source control repository (like a Git repository), Jenkins automatically checks out the repository that contains the Jenkinsfile to execute the pipeline. Since Jenkins has already checked out the repository to read the Jenkinsfile, performing an additional checkout step in the pipeline is redundant in many cases.

important>>
whenever you install plugins restart jenkins server,
when u update credentials in jenkins,NO00 need to restart server. 

jenkins build with parameters
====
In Jenkins, Build with Parameters is a feature that allows you to run a Jenkins job (or pipeline) with custom inputs provided by the user. These inputs are typically defined as parameters, which can be passed to the job at runtime to modify the behavior of the build.
Example Use Cases:
Branch Selection: You can create a parameterized job that builds a specific branch based on user input. The parameter could be BRANCH_NAME, and the user selects the branch they want to build.
Environment Deployment: You can define a DEPLOY_ENV parameter to choose the target environment (e.g., dev, staging, production) where the application should be deployed.
Versioning: Users can input the application version they want to build, allowing for easier version control during testing and deployment.


Declrative syntax jenkins pipeline>>groovy expersions(language)

pipeline as codebuild

pipeline stage view plugin 



kodekloud
=============
Username: admin

Password: Adm!n321

UserName: mike

Password: M!k3@123

Username: bob

Password: caleston123
==============

check jenkins in or with aws 
jenkins cli
==========
jenkins-cli.jar
java -jar jenkins-cli.jar -s http://localhost:8085 -auth 'admin:Adm!n321' <command>
java -jar jenkins-cli.jar -s http://localhost:8085 -auth 'admin:Adm!n321' install-plugin cloudbees-bitbucket-branch-source
java -jar jenkins-cli.jar -s http://localhost:8085 -auth 'admin:Adm!n321' install-plugin cloud-stats(f0r updating)
java -jar jenkins-cli.jar -s http://localhost:8085 -auth 'admin:Adm!n321' disable-plugin github -restart -r
sudo service jenkins restart
sudo systemctl enable jenkins

curl -Lv http://localhost:8085/login 2>&1 | grep -i 'x-ssh-endpoint'


jenkins --help

jenkins plugins
===============
check aws jenkins plugins(codebuild)
sudo systemctl restart jenkins
java -jar jenkins-cli.jar -s http://localhost:8085 -auth 'admin:Adm!n321' restart


Users in jenkins
==============

Backups
=======
Filesystem snapshots
plugins for Backups
shell script that backups jenkins instance
$JENKINS_HOME variable holds the value of Jenkins home directory, which is /var/lib/jenkins in most of the cases but nor necessarily. This is where Jenkins stores its data like jobs, plugins, configs etc
Jenkins home directory is most important to backup, since it contains all jobs, configuration, build history, plugins etc.


jenkins monitoring with prometheus
==============================
- job_name: 'Jenkins'
  metrics_path: /prometheus/
  static_configs:
    - targets: ['localhost:8085']
service prometheus restart

prometheus --config.file=/etc/prometheus/prometheus.yml --test


Jenkins file
================



Build Agent
================
A build agent is typically a machine, or container, which connects to a Jenkins controller and executes tasks when directed by the controller.
Yes, we can use a docker container as well, as an agent node for a Jenkins server.


master-slave(many nodes-join the nodes)

Commonly used tools commands and their purposes in Jenkins:
Global Tool Configuration: Jenkins allows you to configure tools globally that can be used across all jobs. You can access it via:

Jenkins UI → Manage Jenkins → Global Tool Configuration. In this section, you can configure tools like JDK, Maven, Git, Node.js, Gradle, etc.
Declarative Pipeline Syntax for tools: Jenkins pipelines (especially declarative pipelines) allow you to declare the use of certain tools within your pipeline script. This ensures that the right version of the tool is available when the build runs.

Example pipeline using tools:(below is grrovy)
pipeline {
    agent any
    tools {
        jdk 'JDK11'     // Use a specific JDK installed globally in Jenkins
        maven 'Maven3'   // Use Maven version configured in Global Tool Configuration
    }
    stages {
        stage('Build') {
            steps {
                sh 'mvn clean install'  // Maven will use the defined 'Maven3' tool
            }
        }
    }
}

important>>use docker agent
A Docker agent in the context of Jenkins pipelines refers to a Docker container that acts as an agent (or node) to run the build jobs. Instead of running the build on a dedicated Jenkins agent machine, the build is executed within a Docker container. The docker block inside the Jenkins pipeline specifies that the build should use a Docker image as the execution environment.
Steps to Use ECR Docker Agent Image in Jenkins Pipeline
Once your Docker image is in ECR, you can use it in your Jenkins pipeline. Here’s how you can pull the image from ECR and use it as the Docker agent.

Authenticate Jenkins to AWS ECR: You need to ensure that Jenkins can authenticate to AWS to pull Docker images from ECR. There are two common ways to do this:

Option 1: IAM Role (if Jenkins is running on an EC2 instance): Assign an IAM role with the necessary ECR permissions (e.g., ecr:BatchCheckLayerAvailability, ecr:GetDownloadUrlForLayer, and ecr:DescribeRepositories) to the EC2 instance that is running Jenkins.

Option 2: AWS Credentials: You can configure AWS credentials (access key and secret key) in Jenkins using the AWS Credentials Plugin.

Add ECR Login Step in Jenkinsfile: In the Jenkins pipeline, use the following step to log in to ECR before pulling the Docker image.


Built-In Node
Mark this node temporarily offline
This is the Jenkins controller's built-in node. Builds running on this node will execute on the same system and as the same user as the Jenkins controller. This is appropriate e.g. for special jobs performing backups, but in general you should run builds on agents. Learn more about distributed builds.


jenkins with docker 
===========
jenkins server should have docker credentials to login/push to dockerhub 
using jenkins to build docker iamge and push to ECR or docker hub 
my-app/
├── Dockerfile
├── package.json
├── package-lock.json
├── src/
│   └── app.js
└── .dockerignore
Example .dockerignore:
node_modules
.git
Dockerfile

if Docker file is in Docker folder(not in root dir) then use 
docker build -t my-img1 -f Docker/Dockerfile .
my-project/
│
├── src/
│   ├── index.js
│   ├── app.js
│   └── ... (other source files)
│
├── Docker/
│   └── Dockerfile
│
└── package.json


The package.json file contains all the necessary information about your application's dependencies (e.g., libraries, frameworks) and the scripts used to run or build the app.
When you run npm install (or yarn install), Node.js uses the package.json file to know which dependencies to install.

The CMD instruction sets the default command to be executed when the container runs.
It’s helpful because it simplifies running the container. Instead of manually specifying the command each time you run the container, Docker will automatically execute the command defined in CMD.

If you don't have CMD in your Dockerfile, you would need to start the container like this manually:
docker run mynodeapp npm start
with cmd:docker run mynodeapp


important>>
In the case where you define EXPOSE 8080 in the Dockerfile but specify containerPort: 3000 in the Kubernetes Pod spec, it won't cause a runtime error, but it will create a misconfiguration that could lead to the application not being accessible or reachable on the expected port.

docker run -p 3000:8080 mynodeapp

EXPOSE 3000
==========
This line simply documents that your application inside the container is expected to listen on port 3000. However, this does not force the container to listen on port 3000. It is just metadata that indicates that the application expects to receive traffic on that port.


our Node.js application is listening on port 3000 inside the container (e.g., in your code you have app.listen(3000)).

General Rule:
The right side of -p (the container port) must match the port your application is listening on inside the container. The left side of -p (the host port) can be anything you choose, but it will map to the specific port inside the container.

There is no direct docker image rename command to rename an existing Docker image. Instead, you need to tag the existing image with a new name. The docker tag command allows you to "rename" an image by assigning it a new tag.
 
docker tag old-image-name:v1 new-image-name:v1 
